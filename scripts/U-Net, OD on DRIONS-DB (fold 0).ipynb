{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of modified U-Net for Optic Disc on DRIONS-DB database, 256 px images (cross-validation fold #0).\n",
    "\n",
    "You can either train your model or upload a pre-trained one from:\n",
    "*../models_weights/05.03,02:40,U-Net light, on DRIONS-DB 256 px fold 0, SGD, high augm, CLAHE, log_dice loss/last_checkpoint.hdf5*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86150\\Anaconda3\\envs\\yjiang3.7\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\86150\\Anaconda3\\envs\\yjiang3.7\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\86150\\Anaconda3\\envs\\yjiang3.7\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\86150\\Anaconda3\\envs\\yjiang3.7\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\86150\\Anaconda3\\envs\\yjiang3.7\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\86150\\Anaconda3\\envs\\yjiang3.7\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\86150\\Anaconda3\\envs\\yjiang3.7\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\86150\\Anaconda3\\envs\\yjiang3.7\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\86150\\Anaconda3\\envs\\yjiang3.7\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\86150\\Anaconda3\\envs\\yjiang3.7\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\86150\\Anaconda3\\envs\\yjiang3.7\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\86150\\Anaconda3\\envs\\yjiang3.7\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#import warnings\n",
    "#warnings.simplefilter('ignore')\n",
    "import numpy as np\n",
    "import skimage\n",
    "import skimage.exposure\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "%matplotlib inline\n",
    "import h5py\n",
    "from dual_IDG import DualImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import  Model\n",
    "from keras.layers import  Dropout,   Conv2D, MaxPooling2D,  Input,  UpSampling2D,  Concatenate\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint,   CSVLogger\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: 2.3.1\n",
      "TensorFlow version: 1.14.0\n"
     ]
    }
   ],
   "source": [
    "print('Keras version:', keras.__version__)\n",
    "print('TensorFlow version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_image_data_format('channels_first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_IOU_gpu(X, Y):\n",
    "    \"\"\"Computes mean Intersection-over-Union (IOU) for two arrays of binary images.\n",
    "    Assuming X and Y are of shape (n_images, w, h).\"\"\"\n",
    "    \n",
    "    #X_fl = K.clip(K.batch_flatten(X), K.epsilon(), 1.)\n",
    "    #Y_fl = K.clip(K.batch_flatten(Y), K.epsilon(), 1.)\n",
    "    X_fl = K.clip(K.batch_flatten(X), 0., 1.)\n",
    "    Y_fl = K.clip(K.batch_flatten(Y), 0., 1.)\n",
    "    X_fl = K.cast(K.greater(X_fl, 0.5), 'float32')\n",
    "    Y_fl = K.cast(K.greater(Y_fl, 0.5), 'float32')\n",
    "\n",
    "    intersection = K.sum(X_fl * Y_fl, axis=1)\n",
    "    union = K.sum(K.maximum(X_fl, Y_fl), axis=1)\n",
    "    # if union == 0, it follows that intersection == 0 => score should be 0.\n",
    "    union = K.switch(K.equal(union, 0), K.ones_like(union), union)\n",
    "    return K.mean(intersection / K.cast(union, 'float32'))\n",
    "\n",
    "\n",
    "def mean_IOU_gpu_loss(X, Y):\n",
    "    return -mean_IOU_gpu(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice(y_true, y_pred):\n",
    "    # Workaround for shape bug. For some reason y_true shape was not being set correctly\n",
    "    #y_true.set_shape(y_pred.get_shape())\n",
    "\n",
    "    # Without K.clip, K.sum() behaves differently when compared to np.count_nonzero()\n",
    "    #y_true_f = K.clip(K.batch_flatten(y_true), K.epsilon(), 1.)\n",
    "    #y_pred_f = K.clip(K.batch_flatten(y_pred), K.epsilon(), 1.)\n",
    "    y_true_f = K.clip(K.batch_flatten(y_true), 0., 1.)\n",
    "    y_pred_f = K.clip(K.batch_flatten(y_pred), 0., 1.)\n",
    "    #y_pred_f = K.greater(y_pred_f, 0.5)\n",
    "\n",
    "    intersection = 2 * K.sum(y_true_f * y_pred_f, axis=1)\n",
    "    union = K.sum(y_true_f * y_true_f, axis=1) + K.sum(y_pred_f * y_pred_f, axis=1)\n",
    "    return K.mean(intersection / union)\n",
    "\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return -dice(y_true, y_pred)\n",
    "\n",
    "\n",
    "def log_dice_loss(y_true, y_pred):\n",
    "    return -K.log(dice(y_true, y_pred))\n",
    "\n",
    "\n",
    "def dice_metric(y_true, y_pred):\n",
    "    \"\"\"An exact Dice score for binary tensors.\"\"\"\n",
    "    y_true_f = K.cast(K.greater(y_true, 0.5), 'float32')\n",
    "    y_pred_f = K.cast(K.greater(y_pred, 0.5), 'float32')\n",
    "    return dice(y_true_f, y_pred_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_to_th_encoding(X):\n",
    "    return np.rollaxis(X, 3, 1)\n",
    "\n",
    "\n",
    "def th_to_tf_encoding(X):\n",
    "    return np.rollaxis(X, 1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h5f = h5py.File(os.path.join(os.path.dirname(os.getcwd()), 'data', 'hdf5_datasets', 'all_data.hdf5'), 'r')\n",
    "h5f = h5py.File(os.path.join(os.path.dirname(os.getcwd()), 'data', 'hdf5_datasets', 'DRIONS_DB.hdf5'), 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net architecture\n",
    "\n",
    "<img src=\"../pics/u_net_arch.png\" width=80%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet_light(img_rows=256, img_cols=256):\n",
    "    inputs = Input((3, img_rows, img_cols))\n",
    "\n",
    "    # contract path\n",
    "    conv1 = Conv2D(32, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = Dropout(0.3)(conv1)\n",
    "    conv1 = Conv2D(32, kernel_size=3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    print(\"pool1\", pool1.get_shape())\n",
    "\n",
    "    conv2 = Conv2D(64, kernel_size=3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = Dropout(0.3)(conv2)\n",
    "    conv2 = Conv2D(64, kernel_size=3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    print(\"pool2\", pool2.get_shape())\n",
    "\n",
    "    conv3 = Conv2D(64, kernel_size=3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = Dropout(0.3)(conv3)\n",
    "    conv3 = Conv2D(64, kernel_size=3, activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    print(\"pool3\", pool3.get_shape())\n",
    "\n",
    "    conv4 = Conv2D(64, kernel_size=3, activation='relu', padding='same')(pool3)\n",
    "    conv4 = Dropout(0.3)(conv4)\n",
    "    conv4 = Conv2D(64, kernel_size=3, activation='relu', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    print(\"pool4\", pool4.get_shape())\n",
    "\n",
    "    # bottom\n",
    "    conv5 = Conv2D(64, kernel_size=3, activation='relu', padding='same')(pool4)\n",
    "    conv5 = Dropout(0.3)(conv5)\n",
    "    conv5 = Conv2D(64, kernel_size=3, activation='relu', padding='same')(conv5)\n",
    "    print(\"conv5\", conv5.get_shape())\n",
    "\n",
    "    # expansive path\n",
    "    up6 = Concatenate(axis=1)([UpSampling2D(size=(2, 2))(conv5), conv4])\n",
    "    conv6 = Conv2D(64, kernel_size=3, activation='relu', padding='same')(up6)\n",
    "    conv6 = Dropout(0.3)(conv6)\n",
    "    conv6 = Conv2D(64, kernel_size=3, activation='relu', padding='same')(conv6)\n",
    "    print(\"conv6\", conv6.get_shape())\n",
    "\n",
    "    up7 = Concatenate(axis=1)([UpSampling2D(size=(2, 2))(conv6), conv3])\n",
    "    conv7 = Conv2D(64, kernel_size=3, activation='relu', padding='same')(up7)\n",
    "    conv7 = Dropout(0.3)(conv7)\n",
    "    conv7 = Conv2D(64, kernel_size=3, activation='relu', padding='same')(conv7)\n",
    "    print(\"conv7\", conv7.get_shape())\n",
    "\n",
    "    up8 = Concatenate(axis=1)([UpSampling2D(size=(2, 2))(conv7), conv2])\n",
    "    conv8 = Conv2D(64, kernel_size=3, activation='relu', padding='same')(up8)\n",
    "    conv8 = Dropout(0.3)(conv8)\n",
    "    conv8 = Conv2D(64, kernel_size=3, activation='relu', padding='same')(conv8)\n",
    "    print(\"conv8\", conv8.get_shape())\n",
    "\n",
    "    up9 = Concatenate(axis=1)([UpSampling2D(size=(2, 2))(conv8), conv1])\n",
    "    conv9 = Conv2D(32, kernel_size=3, activation='relu', padding='same')(up9)\n",
    "    conv9 = Dropout(0.3)(conv9)\n",
    "    conv9 = Conv2D(32, kernel_size=3, activation='relu', padding='same')(conv9)\n",
    "    print(\"conv9\", conv9.get_shape())\n",
    "\n",
    "    conv10 = Conv2D(1, kernel_size=1, activation='sigmoid', padding='same')(conv9)\n",
    "    print(\"conv10\", conv10.get_shape())\n",
    "    #conv10 = Flatten()(conv10)\n",
    "\n",
    "    model = Model(input=inputs, output=conv10)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\86150\\Anaconda3\\envs\\yjiang3.7\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "pool1 (?, 32, 128, 128)\n",
      "pool2 (?, 64, 64, 64)\n",
      "pool3 (?, 64, 32, 32)\n",
      "pool4 (?, 64, 16, 16)\n",
      "conv5 (?, 64, 16, 16)\n",
      "conv6 (?, 64, 32, 32)\n",
      "conv7 (?, 64, 64, 64)\n",
      "conv8 (?, 64, 128, 128)\n",
      "conv9 (?, 32, 256, 256)\n",
      "conv10 (?, 1, 256, 256)\n",
      "WARNING:tensorflow:From C:\\Users\\86150\\Anaconda3\\envs\\yjiang3.7\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3172: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 3, 256, 256)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 256, 256) 896         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32, 256, 256) 0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 256, 256) 9248        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 128, 128) 0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 128, 128) 18496       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64, 128, 128) 0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 128, 128) 36928       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 64, 64, 64)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 64, 64)   36928       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 64, 64, 64)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 64, 64)   36928       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 64, 32, 32)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 64, 32, 32)   36928       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 64, 32, 32)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 32, 32)   36928       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 64, 16, 16)   0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 64, 16, 16)   36928       max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 64, 16, 16)   0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 64, 16, 16)   36928       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 64, 32, 32)   0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128, 32, 32)  0           up_sampling2d_1[0][0]            \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 64, 32, 32)   73792       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 64, 32, 32)   0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 64, 32, 32)   36928       dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 64, 64, 64)   0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128, 64, 64)  0           up_sampling2d_2[0][0]            \n",
      "                                                                 conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 64, 64, 64)   73792       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 64, 64, 64)   0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 64)   36928       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 64, 128, 128) 0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 128, 128 0           up_sampling2d_3[0][0]            \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 128, 128) 73792       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 64, 128, 128) 0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 64, 128, 128) 36928       dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 64, 256, 256) 0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 96, 256, 256) 0           up_sampling2d_4[0][0]            \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 256, 256) 27680       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 32, 256, 256) 0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 32, 256, 256) 9248        dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 1, 256, 256)  33          conv2d_18[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 656,257\n",
      "Trainable params: 656,257\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86150\\Anaconda3\\envs\\yjiang3.7\\lib\\site-packages\\ipykernel_launcher.py:64: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n"
     ]
    }
   ],
   "source": [
    "model = get_unet_light(img_rows=256, img_cols=256)\n",
    "model.compile(optimizer=SGD(lr=3e-4, momentum=0.95),\n",
    "              loss=log_dice_loss,\n",
    "              metrics=[mean_IOU_gpu, dice_metric])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DRIONS-DB\n",
    "\n",
    "Accessing data, preparing train/validation sets division:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = h5f['DRIONS-DB/256 px/images']\n",
    "Y = h5f['DRIONS-DB/256 px/disc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(<HDF5 dataset \"images\": shape (110, 256, 256, 3), type \"|u1\">,\n <HDF5 dataset \"disc\": shape (110, 256, 256, 1), type \"|u1\">)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Setting a random_state has no effect since shuffle is False. You should leave random_state to its default (None), or set shuffle=True.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-14-b3aec7b216d9>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mtrain_idx_cv\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_idx_cv\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[1;32mfor\u001B[0m \u001B[0m_train_idx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_test_idx\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mKFold\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mn_splits\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m5\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrandom_state\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_train_idx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_test_idx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[0mtrain_idx_cv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_train_idx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\yjiang3.7\\lib\\site-packages\\sklearn\\utils\\validation.py\u001B[0m in \u001B[0;36minner_f\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     61\u001B[0m             \u001B[0mextra_args\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mall_args\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     62\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mextra_args\u001B[0m \u001B[1;33m<=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 63\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     64\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     65\u001B[0m             \u001B[1;31m# extra_args > 0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\yjiang3.7\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, n_splits, shuffle, random_state)\u001B[0m\n\u001B[0;32m    427\u001B[0m                  random_state=None):\n\u001B[0;32m    428\u001B[0m         super().__init__(n_splits=n_splits, shuffle=shuffle,\n\u001B[1;32m--> 429\u001B[1;33m                          random_state=random_state)\n\u001B[0m\u001B[0;32m    430\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    431\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_iter_test_indices\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgroups\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\yjiang3.7\\lib\\site-packages\\sklearn\\utils\\validation.py\u001B[0m in \u001B[0;36minner_f\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     61\u001B[0m             \u001B[0mextra_args\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mall_args\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     62\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mextra_args\u001B[0m \u001B[1;33m<=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 63\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     64\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     65\u001B[0m             \u001B[1;31m# extra_args > 0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\yjiang3.7\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, n_splits, shuffle, random_state)\u001B[0m\n\u001B[0;32m    289\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mshuffle\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mrandom_state\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# None is the default\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    290\u001B[0m             raise ValueError(\n\u001B[1;32m--> 291\u001B[1;33m                 \u001B[1;34m'Setting a random_state has no effect since shuffle is '\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    292\u001B[0m                 \u001B[1;34m'False. You should leave '\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    293\u001B[0m                 \u001B[1;34m'random_state to its default (None), or set shuffle=True.'\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: Setting a random_state has no effect since shuffle is False. You should leave random_state to its default (None), or set shuffle=True."
     ]
    }
   ],
   "source": [
    "train_idx_cv, test_idx_cv = [], []\n",
    "\n",
    "for _train_idx, _test_idx in KFold(n_splits=5, random_state=1).split(X):\n",
    "    print(_train_idx, _test_idx)\n",
    "    train_idx_cv.append(_train_idx)\n",
    "    test_idx_cv.append(_test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_idx = h5f['RIM-ONE v3/train_idx_driu']\n",
    "#test_idx = h5f['RIM-ONE v3/test_idx_driu']\n",
    "\n",
    "train_idx = train_idx_cv[0]\n",
    "test_idx = test_idx_cv[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X), len(train_idx), len(test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator of augmented data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idg = DualImageDataGenerator(#rescale=1/255.0,\n",
    "                                   #samplewise_center=True, samplewise_std_normalization=True,\n",
    "                                   horizontal_flip=True, vertical_flip=True,\n",
    "                                   rotation_range=50, width_shift_range=0.15, height_shift_range=0.15,\n",
    "                                   zoom_range=(0.7, 1.3),\n",
    "                                   fill_mode='constant', cval=0.0)\n",
    "test_idg = DualImageDataGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing function and data generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(batch_X, batch_y, train_or_test='train'):\n",
    "    batch_X = batch_X / 255.0\n",
    "    batch_y = batch_y / 255.0\n",
    "    if train_or_test == 'train':\n",
    "        batch_X, batch_y = next(train_idg.flow(batch_X, batch_y, batch_size=len(batch_X), shuffle=False))\n",
    "    elif train_or_test == 'test':\n",
    "        batch_X, batch_y = next(test_idg.flow(batch_X, batch_y, batch_size=len(batch_X), shuffle=False))\n",
    "    batch_X = th_to_tf_encoding(batch_X)\n",
    "    batch_X = [skimage.exposure.equalize_adapthist(batch_X[i]) \n",
    "               for i in range(len(batch_X))]\n",
    "    batch_X = np.array(batch_X)\n",
    "    batch_X = tf_to_th_encoding(batch_X)\n",
    "    return batch_X, batch_y\n",
    "\n",
    "\n",
    "def data_generator(X, y, train_or_test='train', batch_size=3, return_orig=False, stationary=False):\n",
    "    while True:\n",
    "        if train_or_test == 'train':\n",
    "            idx = np.random.choice(train_idx, size=batch_size)\n",
    "        elif train_or_test == 'test':\n",
    "            if stationary:\n",
    "                idx = test_idx[:batch_size]\n",
    "            else:\n",
    "                idx = np.random.choice(test_idx, size=batch_size)\n",
    "        batch_X = [X[i] for i in idx]\n",
    "        batch_X = np.array(batch_X).copy()\n",
    "        batch_y = [y[i] for i in idx]\n",
    "        batch_y = np.array(batch_y).copy()\n",
    "        batch_X = tf_to_th_encoding(batch_X)\n",
    "        batch_y = tf_to_th_encoding(batch_y)\n",
    "        if return_orig:\n",
    "            batch_X_orig, batch_Y_orig = batch_X.copy(), batch_y.copy()\n",
    "        \n",
    "        batch_X, batch_y = preprocess(batch_X, batch_y, train_or_test)\n",
    "        \n",
    "        if not return_orig:\n",
    "            yield batch_X, batch_y\n",
    "        else:\n",
    "            yield batch_X, batch_y, batch_X_orig, batch_Y_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the data generator and generator for augmented data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen = data_generator(X, Y, 'train', batch_size=1)\n",
    "batch = next(gen)\n",
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.imshow(np.rollaxis(batch[0][0], 0, 3))\n",
    "#plt.colorbar(mappable=fig)\n",
    "plt.show()\n",
    "plt.imshow(batch[1][0][0], cmap=plt.cm.Greys_r); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_name = \"U-Net light, on DRIONS-DB 256 px fold 0, SGD, high augm, CLAHE, log_dice loss\"\n",
    "weights_folder = os.path.join(os.path.dirname(os.getcwd()), 'models_weights',\n",
    "                              '{},{}'.format(datetime.now().strftime('%d.%m,%H:%M'), arch_name))\n",
    "print(weights_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder(folder_name):\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "    return folder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, Y_valid = next(data_generator(X, Y, train_or_test='test', batch_size=100, stationary=True))\n",
    "plt.imshow(np.rollaxis(X_valid[0], 0, 3)); plt.show()\n",
    "print(X_valid.shape, Y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "If a pretrained model needs to be used, first run \"Loading model\" section below and then go the \"Comprehensive visual check\", skipping this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(data_generator(X, Y, train_or_test='train', batch_size=1), \n",
    "                              steps_per_epoch=99,\n",
    "                              max_queue_size=1,\n",
    "                              \n",
    "                              validation_data=(X_valid, Y_valid),\n",
    "                              #validation_data=data_generator(X, Y, train_or_test='test', batch_size=1),\n",
    "                              #nb_val_samples=100,\n",
    "                              \n",
    "                              epochs=500, verbose=1,\n",
    "                              \n",
    "                              callbacks=[CSVLogger(os.path.join(folder(weights_folder), 'training_log.csv')),\n",
    "                                         #ReduceLROnPlateau(monitor='val_loss', mode='min', factor=0.5, verbose=1, patience=40),\n",
    "                                         ModelCheckpoint(os.path.join(folder(weights_folder),\n",
    "                                               #'weights.ep-{epoch:02d}-val_mean_IOU-{val_mean_IOU_gpu:.2f}_val_loss_{val_loss:.2f}.hdf5',\n",
    "                                               'last_checkpoint.hdf5'),\n",
    "                                               monitor='val_loss', mode='min', save_best_only=True, \n",
    "                                               save_weights_only=False, verbose=0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_iou, pred_dice = [], []\n",
    "\n",
    "for i, img_no in enumerate(test_idx):\n",
    "    print('image #{}'.format(img_no))\n",
    "    img = X[img_no]\n",
    "    batch_X = X_valid[i:i + 1]\n",
    "    batch_y = Y_valid[i:i + 1]\n",
    "    \n",
    "    pred = (model.predict(batch_X)[0, 0] > 0.5).astype(np.float64)\n",
    "    #corr = Y[img_no][..., 0]\n",
    "    corr = th_to_tf_encoding(batch_y)[0, ..., 0]\n",
    "    \n",
    "    # mean filtering:\n",
    "    #pred = mh.mean_filter(pred, Bc=mh.disk(10)) > 0.5\n",
    "    \n",
    "    fig = plt.figure(figsize=(9, 4))\n",
    "    ax = fig.add_subplot(1, 3, 1)\n",
    "    ax.imshow(pred, cmap=plt.cm.Greys_r)\n",
    "    ax.set_title('Predicted')\n",
    "    ax = fig.add_subplot(1, 3, 2)\n",
    "    ax.imshow(corr, cmap=plt.cm.Greys_r)\n",
    "    ax.set_title('Correct')\n",
    "    ax = fig.add_subplot(1, 3, 3)\n",
    "    #ax.imshow(img)\n",
    "    ax.imshow(th_to_tf_encoding(batch_X)[0])\n",
    "    ax.set_title('Image')\n",
    "    plt.show()\n",
    "    \n",
    "    cur_iou = K.eval(mean_IOU_gpu(pred[None, None, ...], corr[None, None, ...]))\n",
    "    cur_dice = K.eval(dice(pred[None, None, ...], corr[None, None, ...]))\n",
    "    print('IOU: {}\\nDice: {}'.format(cur_iou, cur_dice))\n",
    "    pred_iou.append(cur_iou)\n",
    "    pred_dice.append(cur_dice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acquiring scores for the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(pred_iou))\n",
    "print(np.mean(pred_dice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the best and the worst cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img_pred_corr(i, file_suffix):    # i is index of image in test_idx\n",
    "    img_no = test_idx[i]\n",
    "    batch_X = X[img_no:img_no + 1]\n",
    "    batch_X = tf_to_th_encoding(batch_X)\n",
    "    batch_y = Y[img_no:img_no + 1]\n",
    "    batch_y = tf_to_th_encoding(batch_y)\n",
    "    batch_X, batch_y = preprocess(batch_X, batch_y, 'test')\n",
    "    \n",
    "    pred = model.predict(batch_X)[0, 0] > 0.5\n",
    "    #corr = Y[img_no][..., 0]\n",
    "    corr = th_to_tf_encoding(batch_y)[0, ..., 0]\n",
    "    \n",
    "    fig = plt.figure(figsize=(9, 4))\n",
    "    ax = fig.add_subplot(1, 3, 1)\n",
    "    ax.imshow(pred, cmap=plt.cm.Greys_r)\n",
    "    ax.set_title('Predicted')\n",
    "    ax = fig.add_subplot(1, 3, 2)\n",
    "    ax.imshow(corr, cmap=plt.cm.Greys_r)\n",
    "    ax.set_title('Correct')\n",
    "    ax = fig.add_subplot(1, 3, 3)\n",
    "    #ax.imshow(img)\n",
    "    ax.imshow(X[img_no])\n",
    "    ax.set_title('Image')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.imsave('drions_db_fold_0_{}_case_image.png'.format(file_suffix), X[img_no])\n",
    "    plt.imsave('drions_db_fold_0_{}_case_pred.png'.format(file_suffix), pred, cmap=plt.cm.Greys_r)\n",
    "    plt.imsave('drions_db_fold_0_{}_case_corr.png'.format(file_suffix), corr, cmap=plt.cm.Greys_r)\n",
    "\n",
    "\n",
    "best_idx = np.argmax(pred_iou)\n",
    "worst_idx = np.argmin(pred_iou)\n",
    "show_img_pred_corr(best_idx, 'best')\n",
    "print('IOU: {} (best)'.format(pred_iou[best_idx]))\n",
    "show_img_pred_corr(worst_idx, 'worst')\n",
    "print('IOU: {} (worst)'.format(pred_iou[worst_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "load_model = True   # lock\n",
    "if not load_model:\n",
    "    print('load_model == False')\n",
    "else:\n",
    "    # specify file:\n",
    "    #model_path = '../models_weights/01.11,22:38,U-Net on DRIONS-DB 256 px, Adam, augm, log_dice loss/' \\\n",
    "    #    'weights.ep-20-val_mean_IOU-0.81_val_loss_0.08.hdf5'\n",
    "    \n",
    "    # or get the most recently modified file in a folder:\n",
    "    model_folder = os.path.join(os.path.dirname(os.getcwd()), 'models_weights', '05.03,02_40,U-Net light, on DRIONS-DB 256 px fold 0, SGD, high augm, CLAHE, log_dice loss')\n",
    "    \n",
    "    model_path = max(glob.glob(os.path.join(model_folder, '*.hdf5')), key=os.path.getctime)\n",
    "    if load_model and not os.path.exists(model_path):\n",
    "        raise Exception('`model_path` does not exist')\n",
    "    print('Loading weights from', model_path)\n",
    "\n",
    "    if load_model:\n",
    "        #with open(model_path + ' arch.json') as arch_file:\n",
    "        #    json_string = arch_file.read()\n",
    "        #new_model = model_from_json(json_string)\n",
    "        model.load_weights(model_path)\n",
    "    \n",
    "    # Reading log statistics\n",
    "    import pandas as pd\n",
    "    \n",
    "    log_path = os.path.join(model_folder, 'training_log.csv')\n",
    "    if os.path.exists(log_path):\n",
    "        log = pd.read_csv(log_path)\n",
    "        if log['epoch'].dtype != 'int64':\n",
    "            log = log.loc[log.epoch != 'epoch']\n",
    "        print('\\nmax val mean IOU: {}, at row:'.format(log['val_mean_IOU_gpu'].max()))\n",
    "        print(log.loc[log['val_mean_IOU_gpu'].idxmax()])\n",
    "        if 'val_dice_metric' in log.columns:\n",
    "            print('\\n' + 'max val dice_metric: {}, at row:'.format(log['val_dice_metric'].max()))\n",
    "            print(log.loc[log['val_dice_metric'].idxmax()])\n",
    "        if 'val_dice' in log.columns:\n",
    "            print('\\n' + 'max val dice: {}, at row:'.format(log['val_dice'].max()))\n",
    "            print(log.loc[log['val_dice'].idxmax()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}